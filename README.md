# Welcome
<body>
 Wecome to Data Science portfolio. This is an overview on some data science projects I have been exploring recently. <br />
 <p>Connect on Linkedin: <a href="https://www.linkedin.com/in/rosalienoel/">rosalienoel</a><\p>
<h2>House Prices - Regression models in R</h2>

Full analysis: <a href="houseprice.html" title="houseprice.html">House Prices - Regression models in R</a>

 <p>This is a presentation of my solution for the Kaggle challenge <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a>, a classic data science competition.<br />
The dataset is a refreshed version of the famous 'boston house price' data set. It contains observations about the sold prices for residential real estate in Ames, Ohio, for a period between 2006 and 2010, along with a number of house attributes.<br />

My final predictions have been generated using a model using an ensembling of predictive results generated by an Elastic Net, a Support Vector Machine, a Gradient Boosted Tree and a Regression Spline.<br />

My Kaggle submission scored a RMSE of 0.12178 on the logarithm of the Sale price.  
This ranked at the 987th place in the leaderboard (top 24% at the time of submission).  </p>
<p>
  <b>Tags: Machine Learning | R | Regression | Ensembling | XGBoost | Features engineering</b>
  
</p>

<h2>Breast cancer dataset - Classification in Python</h2>

Full analysis: <a href="final.html" title="final.html">Breast cancer Wisconsin - Classification</a>

 <p>This project is based on the Breast Cancer Wisconsin data, published by UCI Machine learning. It is implemented in Python. The diagnosis (Benign/Malignant) must be predicted from a series of cell measurements (concavity, dimensions, symmetry, ...)<br />
 I started by exploring the data, both analytically and visually, before implementing a number of classification algorithms (Logistic regression, Tree classifier, Random Forest, XGBoost, Support Vector Machine and Naive Bayes). 
In a second iteration, I ensembled the 3 best performers with a majority vote classifier. <br />My final model was able to detect all the malignant cases in the sample (recall of 100%). It achieved a global accuracy score of 98%.</p>
<p>
  <b>Tags: Machine Learning | Python | Classification | Data visualisation</b>
  
</p>
  </body>
